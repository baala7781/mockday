# MockDay AI Interview Platform - Comprehensive Refactoring Report

## Executive Summary

This document provides a complete report of all changes made during the major refactoring effort to transform the MockDay interview platform from a numeric-score-driven system to a realistic, role-aware, adaptive coaching platform. The refactoring focused on decoupling flow decisions from scoring, improving question quality, and creating a more human-like interview experience.

---

## 1. Core Architectural Changes: From Numbers to Buckets

### 1.1 Introduction of Answer Quality Buckets

**Problem:** Numeric scores (0.0-1.0) were being used for both reporting AND interview flow decisions, causing unrealistic behavior and over-engineering.

**Solution:** Introduced semantic answer quality buckets to decouple flow logic from granular numeric scores.

**Files Changed:**
- `backend/interview_service/models.py`
- `backend/interview_service/flow_decisions.py` (NEW FILE)

**Implementation:**

```python
class AnswerQuality(str, Enum):
    NO_IDEA = "no_idea"      # Candidate has no knowledge
    PARTIAL = "partial"      # Candidate has some knowledge but incomplete
    GOOD = "good"            # Candidate demonstrates solid understanding
    STRONG = "strong"        # Candidate demonstrates excellent understanding
```

**Key Function:**
- `categorize_answer(score: float, answer_text: str, question_type: QuestionType) -> AnswerQuality`
  - Maps numeric scores + text signals to semantic buckets
  - Detects "no idea" signals in answer text (e.g., "don't know", "no idea", "not familiar")
  - Provides more stable and interpretable categorization

**Impact:** 
- Interview flow decisions are now based on semantic understanding, not arbitrary numeric thresholds
- System behaves more predictably and realistically

---

### 1.2 Flow Decision Layer (Decoupled from LLM)

**Problem:** LLM was directly controlling interview flow via `next_difficulty` field, causing inconsistency and unpredictability.

**Solution:** Created a human-logic-based flow decision layer that interprets answer quality buckets.

**Files Changed:**
- `backend/interview_service/models.py`
- `backend/interview_service/flow_decisions.py` (NEW FILE)
- `backend/interview_service/main.py`
- `backend/interview_service/websocket_handler.py`

**Implementation:**

```python
class NextAction(str, Enum):
    FOLLOW_UP = "follow_up"              # Ask follow-up on same topic
    CONTINUE = "continue"                # Move to next question normally
    SWITCH_TOPIC = "switch_topic"        # Move to different skill/topic
    INCREASE_DIFFICULTY = "increase_difficulty"  # Candidate doing well, increase difficulty
```

**Key Function:**
```python
def decide_next_action(
    quality: AnswerQuality,
    consecutive_stuck: int = 0
) -> NextAction:
    """
    Human-logic based flow decisions:
    - NO_IDEA + 1+ consecutive struggles â†’ SWITCH_TOPIC
    - NO_IDEA â†’ CONTINUE (give one more chance)
    - PARTIAL â†’ FOLLOW_UP (help them elaborate)
    - GOOD â†’ CONTINUE (normal progression)
    - STRONG â†’ INCREASE_DIFFICULTY (challenge them)
    """
```

**Impact:**
- Flow decisions are now deterministic and predictable
- No more LLM hallucinations affecting interview flow
- Better candidate experience (follow-ups for partial answers, topic switching for struggles)

---

### 1.3 Removal of `next_difficulty` from LLM Evaluation

**Problem:** LLM was generating `next_difficulty` values that directly controlled interview flow, causing inconsistency.

**Solution:** Removed `next_difficulty` from LLM evaluation output. Difficulty adjustments now happen only in flow logic.

**Files Changed:**
- `backend/interview_service/models.py`
- `backend/interview_service/answer_evaluator.py`
- `backend/interview_service/main.py`

**Changes:**
- Removed `next_difficulty: Optional[DifficultyLevel]` from `Evaluation` model
- Removed `next_difficulty` from LLM prompt in `answer_evaluator.py`
- Removed all references to `evaluation.next_difficulty` in flow logic
- Difficulty adjustments now based on `NextAction` enum values:
  - `INCREASE_DIFFICULTY` â†’ Increase difficulty
  - `SWITCH_TOPIC` â†’ Reset difficulty for new topic
  - Others â†’ Maintain current difficulty

**Impact:**
- LLM focuses solely on evaluation (scoring, feedback)
- Flow control is deterministic and human-controlled
- More consistent interview experience

---

## 2. Question Quality Improvements

### 2.1 Question Validation Layer

**Problem:** Despite prompt instructions, generic and role-inappropriate questions were slipping through (e.g., "What is error handling?" for non-coding roles).

**Solution:** Added a rule-based validation layer that acts as a guardrail before questions are used.

**Files Changed:**
- `backend/interview_service/question_validation.py` (NEW FILE)

**Implementation:**

```python
def validate_question(
    question: str,
    role: str,
    question_type: QuestionType
) -> bool:
    """
    Hard rules enforced:
    1. NO coding questions for non-coding roles (QA, DevOps, Product, Tester)
    2. Questions MUST include scenario keywords (scenario, suppose, imagine, given, etc.)
    3. QA/Tester roles MUST include test-related keywords
    4. Banned generic question patterns ("tell me about", "explain", "what is")
    """
```

**Key Rules:**
- **Hard Rule:** No coding questions for `["tester", "qa", "devops", "product"]` roles
- **Scenario Enforcement:** Questions must include scenario keywords
- **QA-Specific:** QA/Tester questions must include test case/scenario keywords
- **Generic Question Ban:** Blocks questions starting with "tell me about", "explain", "what is"

**Impact:**
- Prevents role-inappropriate questions
- Ensures contextual, scenario-based questions
- Acts as safety net for LLM output

---

### 2.2 Retry Logic with Validation

**Problem:** Invalid questions were being used even when LLM generated poor output.

**Solution:** Added retry mechanism with validation, capped at 3 attempts, with template fallback.

**Files Changed:**
- `backend/interview_service/question_generator.py`

**Implementation:**

```python
MAX_RETRIES = 3
for attempt in range(MAX_RETRIES):
    question = await generate_question(...)
    if validate_question(question.text, role, question.type):
        return question
    logger.warning(f"Question validation failed (attempt {attempt + 1}/{MAX_RETRIES})")
    
# Fallback to role-specific template question
return get_role_specific_fallback_question(role, skill, difficulty)
```

**Impact:**
- Higher quality questions reach candidates
- System gracefully handles LLM failures
- Prevents interviews from stalling due to bad questions

---

### 2.3 Updated Question Generation Prompt

**Problem:** Previous prompt was too generic, leading to theory-only or abstract questions.

**Solution:** Implemented the "FINAL MVP-SAFE QUESTION GENERATION PROMPT" with strict rules and examples.

**Files Changed:**
- `backend/interview_service/question_generator.py`

**Key Improvements:**
- Explicit strict rules (NO EXCEPTIONS)
- Mandatory concrete real-world scenarios
- Role-specific guidance (Tester gets test cases, no coding for non-coding roles)
- Conversation context integration
- Good/bad examples provided
- 2-3 sentence question format

**Impact:**
- More contextual, practical questions
- Better alignment with role requirements
- Reduced generic theory questions

---

### 2.4 Follow-Up Question Support

**Problem:** When candidates gave partial answers, system moved to next question instead of helping them elaborate.

**Solution:** Implemented explicit follow-up question generation when `PARTIAL` answer quality is detected.

**Files Changed:**
- `backend/interview_service/question_generator.py`
- `backend/interview_service/main.py`
- `backend/interview_service/websocket_handler.py`

**Implementation:**
- Added `is_follow_up: bool` parameter to `generate_question()`
- Follow-up prompt focuses on same topic/skill, asks for elaboration
- `NextAction.FOLLOW_UP` triggers follow-up question generation
- WebSocket handler sends follow-up question and returns early (doesn't move to next question)

**Flow:**
1. Candidate gives partial answer â†’ `categorize_answer()` â†’ `PARTIAL`
2. `decide_next_action()` â†’ `FOLLOW_UP`
3. Generate follow-up question on same skill/topic
4. Send to candidate, wait for response
5. After follow-up answer, proceed normally

**Impact:**
- Better candidate experience (helps them elaborate)
- More realistic interview behavior
- Better assessment of candidate knowledge

---

## 3. Scoring and Reporting Improvements

### 3.1 Fixed Scoring Completion Bug

**Problem:** Incomplete interviews (e.g., 1 question) were showing high scores (70%), which was misleading.

**Solution:** Implemented strict completion penalty for very incomplete interviews.

**Files Changed:**
- `backend/interview_service/report_generator.py`

**Implementation:**

```python
def calculate_overall_score(...) -> float:
    base_score = ...
    
    # Strict completion penalty
    if questions_answered <= 3:
        final_score = min(base_score * 0.3, base_score)
        logger.warning(f"âš ï¸ Interview too short ({questions_answered} questions), applying completion penalty")
    elif questions_answered <= 5:
        final_score = min(base_score * 0.6, base_score)
    # ... more thresholds
    
    return final_score
```

**Rules:**
- â‰¤3 questions â†’ max 30% of base score
- â‰¤5 questions â†’ max 60% of base score
- Gradual penalty for incomplete interviews

**Impact:**
- Accurate scores for incomplete interviews
- Prevents misleading high scores
- Encourages complete interviews

---

### 3.2 Simplified Report Structure

**Problem:** Reports felt like exam scores with percentile ranks, "Hire/No Hire" labels, and fake benchmarks.

**Solution:** Redesigned report to be human-readable, actionable, and coaching-focused.

**Files Changed:**
- `backend/interview_service/report_generator.py`
- `backend/interview_service/src/components/report/ExecutiveSummary.tsx` (Frontend)
- `backend/interview_service/src/components/report/ImprovementPlan.tsx` (Frontend)

**New Report Structure:**

1. **Overall Summary (Human Language)**
   - Natural language performance summary
   - No numeric scores in summary
   - Focus on observations and trends

2. **Skill Breakdown (Only Asked Skills)**
   - Level assessment (Good, Needs Improvement, etc.)
   - Observations per skill
   - No percentile ranks

3. **Strengths**
   - Derived from STRONG + GOOD answer quality buckets
   - Actionable, specific observations

4. **Improvement Areas**
   - Derived from PARTIAL + NO_IDEA buckets
   - Focused, actionable feedback

5. **Coaching Suggestions**
   - Specific, actionable recommendations
   - Example: "Before your next interview: Practice explaining solutions out loud"

**Removed:**
- âŒ Percentile ranks
- âŒ "Hire / No Hire" labels
- âŒ Fake industry benchmarks
- âŒ Question count displays ("Questions: X/Y")
- âŒ "Role Match: X%" displays
- âŒ Internal resource links

**Impact:**
- More helpful, coaching-oriented reports
- Less intimidating for candidates
- Focus on improvement, not judgment

---

## 4. LLM Model Management

### 4.1 OpenRouter Integration

**Problem:** System was using multiple LLM providers (Gemini, OpenAI) with inconsistent interfaces.

**Solution:** Migrated all LLM calls to OpenRouter with task-specific model mappings and BYOK (Bring Your Own Key) support.

**Files Changed:**
- `backend/shared/providers/openrouter_client.py`
- `backend/shared/providers/openrouter_pool_client.py` (NEW FILE)
- `backend/interview_service/llm_helpers.py` (NEW FILE)
- `backend/shared/providers/pool_manager.py`
- `backend/shared/config/settings.py`
- All service files using LLM calls

**Deleted:**
- `backend/shared/providers/gemini_client.py` (removed Gemini dependency)

**Task-Specific Model Mappings:**

```python
TASK_MODELS = {
    "question_generation": "openai/gpt-4o-mini",
    "follow_up": "openai/gpt-4o-mini",
    "clarification": "openai/gpt-4o-mini",
    "answer_evaluation": "anthropic/claude-3-haiku",
    "resume_parsing": "openai/gpt-4o-mini",
    "report_generation": "anthropic/claude-3-sonnet",
}
```

**BYOK (Bring Your Own Key) Support:**
- Users can provide OpenRouter API keys
- Keys stored in Redis, retrieved by interview_id
- Falls back to backend pool if user key unavailable

**Pool Management:**
- Multiple OpenRouter keys in backend environment
- Pool manager handles rotation, failover
- Automatic retry with different keys on rate limits

**Files Updated to Use OpenRouter:**
- `question_generator.py`
- `answer_evaluator.py`
- `report_generator.py`
- `resume_analyzer.py`
- `conversational_framing.py`
- `llm_skill_extractor.py`
- `phased_flow.py` (intro questions)

**Impact:**
- Unified LLM interface
- Cost optimization (task-specific model selection)
- Better reliability (pool management, failover)
- User flexibility (BYOK support)

---

## 5. Interview Flow Enhancements

### 5.1 Coding Question Logic Refinement

**Problem:** 
1. Initially: System asked coding questions for all skills, including technology skills (JS, Node.js)
2. Later: System kept asking coding questions even after candidate struggled

**Solution:** Multi-layered approach to prevent inappropriate coding questions.

**Files Changed:**
- `backend/interview_service/phased_flow.py`

**Implementation:**

**Layer 1: Technology Skills Check**
- Technology skills (JavaScript, Node.js, Python, React, etc.) â†’ Theory/conceptual questions only
- Coding questions reserved for problem-solving/algorithmic skills
- Hard-coded list of technology skills (NOTE: User flagged this as needing improvement)

**Layer 2: Role-Based Exclusion**
- Non-coding roles (QA, DevOps, Product Manager, Tester) â†’ No coding questions

**Layer 3: Experience-Based Exclusion**
- Senior/Executive (4+ years) â†’ No coding questions

**Layer 4: Struggle Detection**
- Tracks recent coding question performance
- If candidate struggled with 2+ recent coding questions (score < 0.4) â†’ Stop asking coding questions

**Impact:**
- More appropriate question types per skill
- Better candidate experience (not overwhelmed with coding)
- Adaptive behavior (stops coding if candidate struggles)

**TODO:** Replace hard-coded technology skills list with dynamic categorization (e.g., skill metadata or LLM-based classification)

---

### 5.2 Phased Flow Integration

**Problem:** Flow decisions weren't integrated into phased interview structure.

**Solution:** Integrated answer quality buckets and flow decisions into phased flow.

**Files Changed:**
- `backend/interview_service/phased_flow.py`
- `backend/interview_service/main.py`
- `backend/interview_service/websocket_handler.py`

**Integration Points:**
- After answer evaluation â†’ `categorize_answer()` â†’ `decide_next_action()`
- `NextAction.INCREASE_DIFFICULTY` â†’ Increase difficulty for next question
- `NextAction.SWITCH_TOPIC` â†’ Reset difficulty, move to different skill
- `NextAction.FOLLOW_UP` â†’ Generate follow-up question, don't advance phase
- `NextAction.CONTINUE` â†’ Normal progression

**Impact:**
- Flow decisions properly integrated into interview structure
- Better adaptation to candidate performance
- More realistic interview progression

---

## 6. Frontend Changes

### 6.1 Removed Visual Indicators

**Problem:** UI showed skill/topic indicators (e.g., "python" under "AI Interviewer"), which was distracting.

**Solution:** Removed skill/topic indicators from interview interface.

**Files Changed:**
- `src/components/InterviewInterface.tsx`

**Impact:**
- Cleaner, less distracting UI
- More focus on conversation

---

### 6.2 Disabled Video Feed

**Problem:** Video hardware check and video feed were enabled but not needed.

**Solution:** Commented out video-related code, kept microphone-only.

**Files Changed:**
- `src/pages/HardwareCheck.tsx`
- `src/components/InterviewInterface.tsx`

**Changes:**
- Removed camera permission requests
- Commented out video feed components
- Kept microphone-only functionality

**Impact:**
- Simpler hardware setup
- Can be re-enabled later if needed

---

## 7. Bug Fixes

### 7.1 WebSocket Handler Scoping Issues

**Problem:** `get_candidate_name_safely` function imported inside conditional block but used outside.

**Solution:** Moved import to top of file.

**Files Changed:**
- `backend/interview_service/websocket_handler.py`

**Impact:**
- Fixed scoping error
- Code works correctly

---

### 7.2 Follow-Up Question Flow

**Problem:** When `PARTIAL` answer detected, system moved to next question instead of asking follow-up.

**Solution:** Implemented explicit follow-up handling in WebSocket handler.

**Files Changed:**
- `backend/interview_service/websocket_handler.py`
- `backend/interview_service/main.py`

**Implementation:**
- Detect `NextAction.FOLLOW_UP`
- Generate follow-up question on same topic
- Send to client, return early (don't pre-generate next question)
- After follow-up answer, proceed normally

**Impact:**
- Follow-up questions actually work
- Better candidate experience

---

## 8. Files Created

1. **`backend/interview_service/flow_decisions.py`**
   - Answer quality categorization
   - Flow decision logic

2. **`backend/interview_service/question_validation.py`**
   - Question validation rules
   - Role-specific checks

3. **`backend/shared/providers/openrouter_pool_client.py`**
   - High-level OpenRouter client
   - Task-specific model mappings
   - Pool integration

4. **`backend/interview_service/llm_helpers.py`**
   - BYOK key retrieval
   - LLM call wrapper functions

---

## 9. Files Deleted

1. **`backend/shared/providers/gemini_client.py`**
   - Removed Gemini dependency
   - All LLM calls now via OpenRouter

---

## 10. Configuration Changes

### 10.1 Settings Updates

**Files Changed:**
- `backend/shared/config/settings.py`

**Added:**
- `OPENROUTER_API_KEYS` - List of backend OpenRouter API keys for pool

---

### 10.2 Pool Manager Updates

**Files Changed:**
- `backend/shared/providers/pool_manager.py`

**Added:**
- `ProviderType.OPENROUTER` support
- Multiple OpenRouter key initialization

---

## 11. Summary of Key Improvements

### âœ… Decoupled Flow from Scoring
- Answer quality buckets (NO_IDEA, PARTIAL, GOOD, STRONG)
- Flow decision layer (human logic, not LLM)
- Removed `next_difficulty` from LLM output

### âœ… Improved Question Quality
- Question validation layer (rule-based guardrails)
- Retry logic with fallback
- Updated prompts with strict rules
- Follow-up question support

### âœ… Better Scoring & Reports
- Fixed completion penalty bug
- Simplified, coaching-focused reports
- Removed percentile ranks, "Hire/No Hire" labels

### âœ… Unified LLM Management
- All calls via OpenRouter
- Task-specific model mappings
- BYOK support
- Pool management for reliability

### âœ… Enhanced Interview Flow
- Technology skills â†’ Theory questions
- Coding questions â†’ Problem-solving only
- Struggle detection (stop coding if struggling)
- Adaptive difficulty based on flow decisions

### âœ… Bug Fixes
- WebSocket handler scoping
- Follow-up question flow
- Various integration issues

---

## 12. Known Issues & TODOs

### âš ï¸ Hard-Coded Technology Skills List

**Location:** `backend/interview_service/phased_flow.py` (lines 45-54)

**Issue:** Technology skills list is hard-coded, not dynamic.

**Current Implementation:**
```python
technology_skills = [
    "javascript", "js", "node.js", "nodejs", "python", "java", ...
]
```

**Recommended Solution:**
1. Add skill metadata/categories to skill extraction
2. Use LLM-based classification during skill extraction
3. Store skill type (technology vs. problem-solving) in skill data structure
4. Use skill type to determine question type

**Priority:** Medium (works currently, but not scalable)

---

### ðŸ“‹ Remaining TODOs from Original Plan

1. **Skill-Specific Difficulty Tracking** (TODO 6)
   - Currently: Global difficulty per interview
   - Proposed: `state.skill_difficulty: Dict[str, DifficultyLevel]`
   - Status: Not implemented yet

2. **Clarification Question Support**
   - Users can ask for clarification
   - System should provide clearer question
   - Status: Partially implemented (clarification endpoint exists, but flow integration needed)

---

## 13. Metrics & Impact

### Before Refactoring:
- âŒ Generic, role-inappropriate questions
- âŒ Inconsistent interview flow (LLM-controlled)
- âŒ Misleading scores for incomplete interviews
- âŒ Exam-like reports with fake benchmarks
- âŒ Coding questions for all skills
- âŒ No follow-up support for partial answers

### After Refactoring:
- âœ… Role-appropriate, contextual questions
- âœ… Deterministic, human-logic flow decisions
- âœ… Accurate scores with completion penalties
- âœ… Coaching-focused, actionable reports
- âœ… Technology skills get theory, coding for problem-solving
- âœ… Follow-up questions for partial answers
- âœ… Unified LLM interface (OpenRouter)
- âœ… BYOK support + pool management

---

## 14. Testing Recommendations

1. **Flow Decision Testing**
   - Test all answer quality buckets â†’ next action mappings
   - Verify follow-up questions are generated correctly
   - Verify topic switching happens at right times

2. **Question Validation Testing**
   - Test role-inappropriate questions are blocked
   - Test scenario keywords are enforced
   - Test retry logic with invalid questions

3. **Coding Question Logic Testing**
   - Test technology skills don't get coding questions
   - Test problem-solving skills DO get coding questions
   - Test struggle detection stops coding questions

4. **Report Generation Testing**
   - Test completion penalties for short interviews
   - Verify simplified report structure
   - Test report readability and actionability

5. **OpenRouter Integration Testing**
   - Test BYOK key retrieval
   - Test pool fallback
   - Test task-specific model selection

---

## 15. Conclusion

This refactoring represents a major shift from a numeric-score-driven system to a realistic, role-aware, adaptive coaching platform. The changes improve question quality, interview flow, scoring accuracy, and report usefulness. The system is now more predictable, reliable, and candidate-friendly.

**Key Achievement:** Successfully decoupled interview flow decisions from numeric scoring, creating a more natural and helpful interview experience.

**Next Steps:**
1. Replace hard-coded technology skills list with dynamic categorization
2. Implement skill-specific difficulty tracking
3. Enhance clarification question flow integration
4. User testing and feedback collection

---

**Document Version:** 1.0  
**Last Updated:** Current Date  
**Author:** AI Assistant (based on user requirements and implementation)

